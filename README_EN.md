# Video Dubbing - AI-Powered Video Translation Tool

![Python Version](https://img.shields.io/badge/python-3.11-blue)
![License](https://img.shields.io/badge/license-MIT-green)

English | [ÁÆÄ‰Ωì‰∏≠Êñá](./README.md)

Video Dubbing is an AI-driven command-line tool for video translation and dubbing, offering end-to-end automation from Automatic Speech Recognition (ASR), text translation to Text-to-Speech (TTS). It supports batch processing of video files and one-click generation of multilingual dubbed videos.

## ‚ú® Features

- **Comprehensive Functionality**: ASR ‚Üí Translation ‚Üí TTS full-chain automation
  - High-accuracy speech recognition powered by [OpenAI-Whipser](https://github.com/openai/whisper)
  - Support for any OpenAI-compatible LLM API for translation
  - High-quality voice synthesis based on Edge-TTS
- **Highly Flexible Configuration**: Each function can be independently toggled for scenarios such as **batch subtitle translation** and **speech synthesis**
- **Rich Post-processing Options**: Built-in audio track/subtitle addition, supports subtitle format customization
- **Batch Processing**: Supports glob pattern matching for one-click batch processing
- **Multi-Hardware Support**: Runs on CPU/CUDA/NPU platforms

## üöÄ Quick Start

### Configure Environment

**Requirements**: Python 3.11, ffmpeg, ffprobe

Recommended to use the excellent Python environment management tool [astral-sh/uv](https://github.com/astral-sh/uv)

**Install ffmpeg & ffprobe**:

- Windows: Download from [this link](https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-win64-gpl.zip) and extract, copy `bin/ffmpeg.exe` and `bin/ffprobe.exe` to a directory in your `PATH`
- MacOS: `brew install ffmpeg`
- Linux: ...

### Install Video Dubbing

Choose one of the following installation methods:

**Minimal Installation**:

Suitable for scenarios without ASR, requiring only translation and TTS, avoiding installation of large dependencies like `pytorch, pandas`, occupying only ~20MB

```bash
pip install video-dubbing
# or
uv tool install -p 3.11 video-dubbing # with uv
```

**Basic Installation**:

```bash
pip install video-dubbing[asr]
# or
uv tool install -p 3.11 video-dubbing[asr] # with uv
```

**Huawei NPU**:

```bash
pip install video-dubbing[npu]
# or
uv tool install -p 3.11 video-dubbing[npu] # with uv
```

Theoretically, any platform supported by [torch_npu](https://gitee.com/ascend/pytorch) can run this tool. Tested on Ascend 910B3. If you successfully run it on other platforms, you're welcome to submit a PR to update this section.

### Basic Usage

Due to the numerous configuration options, it's recommended to use a configuration file. First, generate a default configuration file:

**Generate Config File**:

```bash
dub -gc # Generates config.json with default settings in the current directory
```

Then when executing, specify the configuration file with the `-c` parameter and add other parameters to override settings in the config file:

**Load Config File**:

```bash
dub -c config.json # Add other command line parameters after this
```

### Examples

**Example 1**: Batch translate all mp4 files in the videos directory to Chinese:

```bash
dub -c config.json --input-videos videos/*.mp4 --use-html
```

**Example 2**: Batch translate subtitles in the subs directory to Chinese:

```bash
dub -c config.json --input-subtitles subs/*.srt --asr False --tts False
```

## ‚ö†Ô∏è Notes

- It's not recommended to batch process many files at once initially. Try with a single file first to ensure correct configuration
- Set `--debug` and `--log_dir` to enable debug mode and save logs for troubleshooting
- When reporting issues, please provide detailed configuration information and logs for faster diagnosis

File limitations:

- Video file format support depends on ffmpeg, common formats like mp4, mkv, webm are all supported
- Subtitle files only support srt format, other formats can be converted to srt using ffmpeg
- Ensure subtitle files are UTF-8 encoded to avoid encoding issues
- If only specifying videos, ASR must be enabled; if only specifying subtitles, ASR cannot be enabled; if both are specified, their numbers and order must correspond one-to-one

NPU related:

- When using NPU, keep `--device` as `cuda`, no need to modify

LLM related:

- The `--use_html` option is recommended when original subtitles have good sentence breaks (i.e., each line ends with a period). Otherwise, it's recommended to turn it off. Subtitles generated by whisper from English videos generally meet this requirement.

## ‚öôÔ∏è All Configuration Parameters

```bash
options:
  -h, --help            show this help message and exit
  -c CONFIG, --config CONFIG
                        Load JSON or YAML format configuration file (default: None)
  -gc, --gen-config     Generate default configuration file (default: False)
  -v, --version         show program's version number and exit

General:
  --input_videos INPUT_VIDEOS [INPUT_VIDEOS ...], --input-videos INPUT_VIDEOS [INPUT_VIDEOS ...]
                        Videos to process (default: [])
  --input_subtitles INPUT_SUBTITLES [INPUT_SUBTITLES ...], --input-subtitles INPUT_SUBTITLES [INPUT_SUBTITLES ...]
                        Subtitle files to process (srt format) (default: [])
  --output_dir OUTPUT_DIR, --output-dir OUTPUT_DIR
                        Output directory. Use the input file directory if not set (default: None)
  --asr [ASR]           Speech recognition switch (default: True)
  --translate [TRANSLATE]
                        Translation switch (default: True)
  --tts [TTS]           Speech synthesis switch (default: True)
  --debug [DEBUG]       Debug mode (default: False)
  --log_dir LOG_DIR, --log-dir LOG_DIR
                        Log directory, if empty logs won't be saved (default: None)

ASR:
  --model MODEL         Whisper model type (default: turbo)
  --model_dir MODEL_DIR, --model-dir MODEL_DIR
                        Whisper model storage directory (default: None)
  --device DEVICE       Hardware device for running ASR models (default: cuda)
  --align [ALIGN]       Perform word alignment (default: False)
  --diarize [DIARIZE]   Perform speaker diarization (default: False)
  --hf_token HF_TOKEN, --hf-token HF_TOKEN
                        Hugging Face token. For downloading models requiring user agreement (default: )

Translate:
  --target_lang TARGET_LANG, --target-lang TARGET_LANG
                        Target language (default: ÁÆÄ‰Ωì‰∏≠Êñá)
  --base_url BASE_URL, --base-url BASE_URL
                        LLM API address (default: https://api.openai.com/v1)
  --api_key API_KEY, --api-key API_KEY
                        LLM API key (default: )
  --llm_model LLM_MODEL, --llm-model LLM_MODEL
                        LLM model (default: )
  --use_html [USE_HTML], --use-html [USE_HTML]
                        Use HTML tags for multi-line translation requests. Recommended when subtitles have good sentence breaks (default: False)
  --remove_ellipsis [REMOVE_ELLIPSIS], --remove-ellipsis [REMOVE_ELLIPSIS]
                        Remove ellipses at the end of subtitle lines (default: False)
  --llm_req_rate LLM_REQ_RATE, --llm-req-rate LLM_REQ_RATE
                        LLM request rate (r/s) (default: 5)
  --batch_size BATCH_SIZE, --batch-size BATCH_SIZE
                        Maximum number of lines per LLM translation request. Too large will increase failure rate (default: 10)

TTS:
  --voice VOICE         TTS voice. See https://gist.github.com/BettyJJ/17cbaa1de96235a7f5773b8690a20462 (default: zh-CN-YunyangNeural)
  --tts_req_rate TTS_REQ_RATE, --tts-req-rate TTS_REQ_RATE
                        TTS request rate (r/10s) (default: 3)
  --audio_format AUDIO_FORMAT, --audio-format AUDIO_FORMAT
                        Audio output format (default: aac)
  --add_track [ADD_TRACK], --add-track [ADD_TRACK]
                        Add TTS audio to video (default: True)
  --track_title TRACK_TITLE, --track-title TRACK_TITLE
                        TTS track title. Default uses voice name (default: None)

Subtitle:
  --soft [SOFT]         Subtitle addition method (True: soft / False: hard) (currently only supports soft subtitles) (default: True)
  --add_asr_sub [ADD_ASR_SUB], --add-asr-sub [ADD_ASR_SUB]
                        Add speech recognition subtitles to video (default: True)
  --asr_sub_title ASR_SUB_TITLE, --asr-sub-title ASR_SUB_TITLE
                        Speech recognition subtitle title (default: None)
  --asr_sub_style ASR_SUB_STYLE, --asr-sub-style ASR_SUB_STYLE
                        Speech recognition subtitle style. See: https://github.com/yuanshanhua/video-dubbing/blob/main/docs/subtitle_style_en.md (default: None)
  --add_trans_sub [ADD_TRANS_SUB], --add-trans-sub [ADD_TRANS_SUB]
                        Add translated subtitles to video (default: True)
  --trans_sub_title TRANS_SUB_TITLE, --trans-sub-title TRANS_SUB_TITLE
                        Translated subtitle title (default: None)
  --trans_sub_style TRANS_SUB_STYLE, --trans-sub-style TRANS_SUB_STYLE
                        Translated subtitle style (default: None)
  --add_bilingual_sub [ADD_BILINGUAL_SUB], --add-bilingual-sub [ADD_BILINGUAL_SUB]
                        Add bilingual subtitles to video (default: True)
  --bilingual_sub_title BILINGUAL_SUB_TITLE, --bilingual-sub-title BILINGUAL_SUB_TITLE
                        Bilingual subtitle title (default: None)
  --bilingual_sub_style BILINGUAL_SUB_STYLE, --bilingual-sub-style BILINGUAL_SUB_STYLE
                        Bilingual subtitle style (default: None)
```

## üôè Acknowledgements

This project is based on the following excellent open-source projects:

- [OpenAI-Whisper](https://github.com/openai/whisper)
- [FFmpeg](https://ffmpeg.org/)
- [whisperX](https://github.com/m-bain/whisperX)
- [edge-tts](https://github.com/rany2/edge-tts)
- [aiolimiter](https://github.com/mjpieters/aiolimiter)
